{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Name: Arindam Choudhury"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # 3. NvidiaLLM_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Install folowing libraries to run this nonebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DSOsRR8ysrnf"
   },
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install gradio\n",
    "# !pip install -qU langchain-openai\n",
    "# !pip install langchain_google_genai\n",
    "# !pip install faiss-cpu\n",
    "# !pip install langchain_openai\n",
    "# !pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 10752,
     "status": "ok",
     "timestamp": 1708873794056,
     "user": {
      "displayName": "Arindam Choudhury",
      "userId": "03640084298538679622"
     },
     "user_tz": 300
    },
    "id": "18WpOAbyPz0Y"
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, GooglePalmEmbeddings\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "import os\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Google API Key Setup Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### You need API key for Google PALM generative LLM to use Google Embedding\n",
    "\n",
    "Before you can use the Gemini API, you must first obtain an API key. If you don't already have one, create a key with one click in Google AI Studio.\n",
    "\n",
    "<a class=\"button button-primary\" href=\"https://makersuite.google.com/app/apikey\" target=\"_blank\" rel=\"noopener noreferrer\">Get an API key</a>\n",
    "\n",
    "In Colab, add the key to the secrets manager under the \"ðŸ”‘\" in the left panel. Give it the name `GOOGLE_API_KEY`.\n",
    "\n",
    "Once you have the API key, pass it to the SDK. You can do this in two ways:\n",
    "\n",
    "* Put the key in the `GOOGLE_API_KEY` environment variable (the SDK will automatically pick it up from there).\n",
    "* Pass the key to `genai.configure(api_key=...)`\n",
    "\n",
    "Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Connect google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22838,
     "status": "ok",
     "timestamp": 1708873816881,
     "user": {
      "displayName": "Arindam Choudhury",
      "userId": "03640084298538679622"
     },
     "user_tz": 300
    },
    "id": "BVpJw5esRYoK",
    "outputId": "bc0dcb15-f00c-41db-ed55-11c21fdd2a68"
   },
   "outputs": [],
   "source": [
    "# from google.colab import userdata, drive\n",
    "# drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1708874050145,
     "user": {
      "displayName": "Arindam Choudhury",
      "userId": "03640084298538679622"
     },
     "user_tz": 300
    },
    "id": "zFelge5qAt5v"
   },
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY               = 'AI-'\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'sk-K'\n",
    "#save_data_folder             = '/content/drive/MyDrive/Arindam_Choudhury_NvidiaLLM/NVIDIA_DATA_FOLDER/'\n",
    "save_data_folder             = './NVIDIA_DATA_FOLDER/'\n",
    "vactor_db_name_MiniLM        = 'FIAS_INDEX_VDB_MiniLM'\n",
    "vactor_db_name_mpnet         = 'FIAS_INDEX_VDB_mpnet'\n",
    "vactor_db_name_google        = 'FIAS_INDEX_VDB_google'\n",
    "vactor_db_name_G_Palm        = 'FIAS_INDEX_VDB_G_Palm'\n",
    "vactor_db_name_openai        = 'FULL_FIAS_INDEX_VDB_openai'\n",
    "Sample_Question              = 'Sample_Question.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Load Sample Questions file (To test the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_df = pd.read_csv(save_data_folder + Sample_Question)\n",
    "QA_df.drop(\"Unnamed: 0\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziLyswE3Sbv1"
   },
   "source": [
    "> #### Define LLM Models and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 379,
     "status": "ok",
     "timestamp": 1708873817258,
     "user": {
      "displayName": "Arindam Choudhury",
      "userId": "03640084298538679622"
     },
     "user_tz": 300
    },
    "id": "9QELW4QssgJP"
   },
   "outputs": [],
   "source": [
    "LLM_PaLM2_text  = GoogleGenerativeAI(model=\"models/text-bison-001\", google_api_key=GOOGLE_API_KEY, temperature=1)\n",
    "#LLM_OpenAI      = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")\n",
    "LLM_Gemini_pro  = GoogleGenerativeAI(model=\"gemini-pro\", google_api_key=GOOGLE_API_KEY, temperature=1)\n",
    "LLM_Gemini_pro1 = GoogleGenerativeAI(model=\"gemini-1.0-pro\", google_api_key=GOOGLE_API_KEY, temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 10789,
     "status": "ok",
     "timestamp": 1708874378100,
     "user": {
      "displayName": "Arindam Choudhury",
      "userId": "03640084298538679622"
     },
     "user_tz": 300
    },
    "id": "FJlHrN4SSaL3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f0b75f9180645bf9fe7d80487956594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55e985854734330b2efb67ecd554566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EMBEDDING_MiniLM = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-l6-v2\", multi_process=True, model_kwargs={'device': 'cpu'})\n",
    "EMBEDDING_mpnet  = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\", multi_process=True, model_kwargs={'device': 'cpu'})\n",
    "EMBEDDING_Google = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", task_type = \"RETRIEVAL_QUERY\", google_api_key=GOOGLE_API_KEY)\n",
    "EMBEDDING_G_PALM = GooglePalmEmbeddings(model=\"models/embedding-gecko-001\", google_api_key=GOOGLE_API_KEY)\n",
    "#EMBEDDING_OpenAI = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSJFbhKjsgJQ"
   },
   "source": [
    "> #### SET MODEL AND EMBEDDING TO USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 385,
     "status": "ok",
     "timestamp": 1708874381852,
     "user": {
      "displayName": "Arindam Choudhury",
      "userId": "03640084298538679622"
     },
     "user_tz": 300
    },
    "id": "n6cJvA6msgJQ"
   },
   "outputs": [],
   "source": [
    "LLM = LLM_PaLM2_text\n",
    "EMBEDDING = EMBEDDING_MiniLM\n",
    "\n",
    "if EMBEDDING == EMBEDDING_MiniLM:\n",
    "    vactor_db_name = vactor_db_name_MiniLM; SEARCH_TYPE = 'mmr';        KWARGS_DOC = 4\n",
    "if EMBEDDING == EMBEDDING_mpnet:\n",
    "    vactor_db_name = vactor_db_name_mpnet;  SEARCH_TYPE = 'similarity'; KWARGS_DOC = 4\n",
    "if EMBEDDING == EMBEDDING_Google:\n",
    "    vactor_db_name = vactor_db_name_google; SEARCH_TYPE = 'mmr';        KWARGS_DOC = 4\n",
    "if EMBEDDING == EMBEDDING_G_PALM:\n",
    "    vactor_db_name = vactor_db_name_G_Palm; SEARCH_TYPE = 'mmr';        KWARGS_DOC = 4\n",
    "# if EMBEDDING == EMBEDDING_OpenAI:\n",
    "#     vactor_db_name = vactor_db_name_openai; SEARCH_TYPE = 'similarity'; KWARGS_DOC = 4\n",
    "\n",
    "VECTORDB  = FAISS.load_local(save_data_folder + vactor_db_name, EMBEDDING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Question Answer PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1708874385066,
     "user": {
      "displayName": "Arindam Choudhury",
      "userId": "03640084298538679622"
     },
     "user_tz": 300
    },
    "id": "6DlA_OsGAt5x"
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "\n",
    "Your task is to answer the Question based only on the Context docuuments.\n",
    "\n",
    "While answering the asked question take your time to read the Context documents and provide as much details as possible from the CONTEXT.\n",
    "\n",
    "Do NOT use any salutation e.g Hi, Hey, Hello or name of a person in the answer.\n",
    "\n",
    "Do NOT add url links at the begining of the answer and Do NOT add README file or github links in the answer.\n",
    "\n",
    "Always format the answer for better readability when applicable.\n",
    "\n",
    "In case you are unable to answer the Question from the Context docuuments then you can say that you don't know the answer and do not makeup an answer.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {input} \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Function: Create Retrieval chain using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27431,
     "status": "ok",
     "timestamp": 1708874415566,
     "user": {
      "displayName": "Arindam Choudhury",
      "userId": "03640084298538679622"
     },
     "user_tz": 300
    },
    "id": "bz5UE-nqAt5x",
    "outputId": "572b23c0-abe6-4493-b909-4244ed5459a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector DB size:  808127\n",
      "LLM : \u001b[1mGoogleGenerativeAI\u001b[0m\n",
      "Params: {'model': 'models/text-bison-001', 'temperature': 1.0, 'top_p': None, 'top_k': None, 'max_output_tokens': None, 'candidate_count': 1}\n",
      "Embedding : client=SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False})\n",
      "  (2): Normalize()\n",
      ") model_name='sentence-transformers/all-MiniLM-l6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={} multi_process=True show_progress=False\n"
     ]
    }
   ],
   "source": [
    "def get_qa_chain(model, prompt):\n",
    "    print(\"Vector DB size: \", VECTORDB.index.ntotal)\n",
    "    print(\"LLM :\", model)\n",
    "    print(\"Embedding :\", EMBEDDING)\n",
    "\n",
    "    retriever = VECTORDB.as_retriever(search_type=SEARCH_TYPE, search_kwargs={\"k\": KWARGS_DOC})\n",
    "    qa_chain_prompt    = PromptTemplate(template = prompt, input_variables = [\"context\", \"input\"])\n",
    "    combine_docs_chain = create_stuff_documents_chain(model, qa_chain_prompt)\n",
    "    retrieval_chain    = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "    return retrieval_chain\n",
    "retrieval_chain  = get_qa_chain(LLM, prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Function: Invoke LLM to Get the Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1708874427122,
     "user": {
      "displayName": "Arindam Choudhury",
      "userId": "03640084298538679622"
     },
     "user_tz": 300
    },
    "id": "dqOkDz3EAt5x"
   },
   "outputs": [],
   "source": [
    "def get_ans_from_llm(retrieval_chain, question):\n",
    "    result = retrieval_chain.invoke({\"input\": question})\n",
    "    return result['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### TESTING CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "RAPIDSâ„¢ is a suite of open-source libraries that accelerate end-to-end data science and analytics pipelines, delivering up to 10x faster performance for data preparation, machine learning inference and training, and graph analytics. Built on CUDAÂ®, RAPIDS provides a unified experience for data scientists, engineers, and analysts to accelerate their workflows on NVIDIA GPUs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import langchain\n",
    "# langchain.debug = True\n",
    "question = \"What is NVIDIA RAPIDS?\"\n",
    "# question = \"What is the NVIDIA CUDA Toolkit?\"\n",
    "# question = \"How can I install the NVIDIA CUDA Toolkit on Windows?\"\n",
    "# question = \"What is Megatron 530B LLM?\"\n",
    "# question = \"Why do I need nvCOMP\"\n",
    "#question = \"What is DeepStream SDK\"\n",
    "result = retrieval_chain.invoke({\"input\": question})\n",
    "Markdown(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Print Answers from the Sample Question File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, QA in QA_df.iterrows():\n",
    "#     result = retrieval_chain.invoke({\"input\": QA[\"Question\"]})\n",
    "#     QA_df.iloc[i,1] = result[\"answer\"]\n",
    "\n",
    "#     format_print = f\"\"\"\n",
    "#     {i+1}. Question: {QA['Question']}\n",
    "#     Answer: {result['answer']}\n",
    "#     {'-' * 100} \"\"\"\n",
    "#     print(format_print)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Do you want to save these Answers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA_df.to_csv(save_data_folder + Sample_Question, columns=[\"Question\", \"Answer\"])\n",
    "#QA_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Function: Graido URL for Interactive Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "executionInfo": {
     "elapsed": 7450,
     "status": "ok",
     "timestamp": 1708874476266,
     "user": {
      "displayName": "Arindam Choudhury",
      "userId": "03640084298538679622"
     },
     "user_tz": 300
    },
    "id": "lMcN_kQ2nNuX",
    "outputId": "efea51d0-faf5-4515-a651-f018f76a5dd2"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgreet\u001b[39m(question):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_ans_from_llm(retrieval_chain, question)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mgr\u001b[49m\u001b[38;5;241m.\u001b[39mBlocks() \u001b[38;5;28;01mas\u001b[39;00m demo:\n\u001b[1;32m     17\u001b[0m     response  \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mTextbox(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m     question  \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mTextbox(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gr' is not defined"
     ]
    }
   ],
   "source": [
    "FAQs = [\n",
    " \"What is NVIDIA?\"\n",
    ",\"What is NVIDIA RAPIDS?\"\n",
    ",\"What is the NVIDIA CUDA Toolkit?\"\n",
    ",\"How can I install the NVIDIA CUDA Toolkit on Windows?\"\n",
    ",\"What is the difference between NVIDIA's BioMegatron and Megatron 530B LLM?\"\n",
    ",\"What is Megatron 530B LLM?\"\n",
    ",\"What is BioMegatron?\"\n",
    ",\"Why do I need nvCOMP\"\n",
    ",\"What is the difference GPU and CPU\"\n",
    ",\"Why do I need GPU\"]\n",
    "\n",
    "def greet(question):\n",
    "    return get_ans_from_llm(retrieval_chain, question)\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    response  = gr.Textbox(label=\"Answer:\")\n",
    "    question  = gr.Textbox(label=\"Question:\")\n",
    "    image = save_data_folder + \"nvidia.png\"\n",
    "    gr.Image(image)\n",
    "    gr.Markdown(\"Ask me a questions related to NVIDIA, I'll try to answer referring SDKs\")\n",
    "    iface = gr.Interface(fn=greet, inputs=question, outputs=response, theme=gr.themes.Glass(),\n",
    "                examples=FAQs,allow_flagging=\"never\")\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
