{"cells":[{"cell_type":"markdown","metadata":{},"source":["* Name: Arindam Choudhury\n","* Uplevel Email Address: arindam.choudhury.email@gmail.com\n","* Problem Statement: Problem-01: AI-Assisted Learning for NVIDIA SDKs and Toolkits"]},{"cell_type":"markdown","metadata":{},"source":["> # 1. AC_NvidiaLLM_helper_webscraping.ipynb\n","### Scraping NVIDIA and other web-pages and get the content text from each link sites:\n","\n","For this project following websites are scraped and their contents were read and save in \"NVIDIA_DATA_FOLDER\"\n","\n","1. NVIDIA Main page                 `https://www.nvidia.com/en-us/` \n","2. NVIDIA Main document page        `https://docs.nvidia.com`\n","3. NVIDIA All document pages        `https://docs.nvidia.com/#all-documents`\n","4. NVIDIA Forum pages               `https://forums.developer.nvidia.com/`\n","5. NVIDIA Customer Help pages       `https://nvidia.custhelp.com/app/answers/list/st/5/kw/grid`\n","6. Wikipedia (NVIDIA) page          `https://en.wikipedia.org/wiki/Nvidia`\n","7. Stack-Overflow pages for NVIDIA  `https://stackoverflow.com/ ` "]},{"cell_type":"markdown","metadata":{"id":"S6xRsR5Kd9FD"},"source":["> #### Import Necessary Libraries"]},{"cell_type":"code","execution_count":70,"metadata":{"id":"18WpOAbyPz0Y"},"outputs":[],"source":["from langchain_community.document_loaders import UnstructuredURLLoader, UnstructuredFileLoader\n","from langchain.docstore.document import Document\n","from unstructured.cleaners.core import remove_punctuation,clean,clean_extra_whitespace, group_broken_paragraphs\n","from bs4 import BeautifulSoup\n","import requests\n","import pickle\n","import re\n","from tqdm import tqdm\n","import time"]},{"cell_type":"markdown","metadata":{},"source":["> #### Connect google drive\n","* ### Make sure to unzip Arindam_Choudhury_NvidiaLLM.zip and save \"MyDrive\" location"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from google.colab import userdata, drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","metadata":{},"source":["> #### Define global variables for this notebook"]},{"cell_type":"code","execution_count":71,"metadata":{"id":"zFelge5qAt5v"},"outputs":[],"source":["save_data_folder = '/content/drive/MyDrive/Arindam_Choudhury_NvidiaLLM/NVIDIA_DATA_FOLDER/'\n","ALL_PAGE_LINKS   = set()                   # Saves all proceesed Links (Used for finding Duplicate)\n","\n","# Webpage comments. Will be removed from the page\n","comment1 = \"Your browser either does not have JavaScript enabled or does not appear to support enough features of JavaScript to be used well on this site. \"\n","comment2 = \"This site requires Javascript in order to view all its content. Please enable Javascript in order to access all the functionality of this web site. \"\n","comment3 = \"Here are the  instructions how to enable JavaScript in your web browser.\""]},{"cell_type":"markdown","metadata":{},"source":["> #### Function: Save and Load file"]},{"cell_type":"code","execution_count":72,"metadata":{"id":"Pw15P8StGPsu"},"outputs":[],"source":["def save_data(file_name, data):\n","    with open(save_data_folder + file_name, 'wb') as file:\n","      pickle.dump(data, file)\n","\n","def load_data(file_name):\n","    with open(save_data_folder + file_name, 'rb') as file:\n","      return pickle.load(file)"]},{"cell_type":"markdown","metadata":{},"source":["> #### Function: Check for duplicate links (Already processed)"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":["def check_duplicate_links(working_page_links):\n","    dup_count = 0\n","    for link in working_page_links:\n","        if link in ALL_PAGE_LINKS:\n","            working_page_links.remove(link)\n","            dup_count += 1\n","        else:\n","            ALL_PAGE_LINKS.add(link)\n","    if dup_count > 0:\n","        print(f\"Number of Duplicate links removed: {dup_count}\")\n","    else:\n","        print(f\"No Duplicate links found\")\n","    return working_page_links"]},{"cell_type":"markdown","metadata":{},"source":["> #### Function: Read page content and create documents"]},{"cell_type":"code","execution_count":74,"metadata":{"id":"7oe-9_IBqFg0"},"outputs":[],"source":["def read_content_from_pages(working_page_links, save_file_name, print_count):\n","    PAGE_CONTENT   = []\n","    load_count     = 1\n","\n","    url_tbp        = working_page_links.pop()\n","    while url_tbp:\n","        url_loader = UnstructuredURLLoader(urls = [url_tbp], \n","                                           mode=\"elements\",\n","                                           continue_on_failure = True, \n","                                           show_progress_bar=False,\n","                                           post_processors=[clean,remove_punctuation,clean_extra_whitespace])\n","        \n","        elements = url_loader.load()\n","                                                        # cleaning the contents\n","        selected_elements = [e for e in elements if e.metadata['category']==\"NarrativeText\"]\n","        full_clean = \" \".join([e.page_content for e in selected_elements])\n","        full_clean = full_clean.replace(comment1, '')\n","        full_clean = full_clean.replace(comment2, '')\n","        full_clean = full_clean.replace(comment2, '')\n","              \n","        PAGE_CONTENT.extend([Document(page_content=full_clean, metadata={\"source\":url_tbp})])\n","        try:\n","            url_tbp = working_page_links.pop()\n","            if  load_count % print_count == 0:\n","                print (f\"Processing completed for {load_count} pages....\")\n","                save_data(save_file_name, PAGE_CONTENT) # SAVE_PAGE_CONTENT\n","                time.sleep(5)\n","            load_count += 1\n","        except:\n","            print (f\"Processing completed for {load_count} pages....\")\n","            save_data(save_file_name, PAGE_CONTENT)\n","            break\n","\n","    return PAGE_CONTENT"]},{"cell_type":"markdown","metadata":{},"source":["> ### 1. NVIDIA Main page                 `https://www.nvidia.com/en-us/`"]},{"cell_type":"markdown","metadata":{},"source":["> #### Function: Get all href links from the page"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[],"source":["def get_links_main_page(url, save_file):    \n","    PAGE_LINKS = set()\n","    html_text = requests.get(url).content\n","    soup = BeautifulSoup(html_text, 'lxml')\n","    links = soup.find_all(\"a\", href=re.compile(\"https://\"))\n","    for link in links:\n","        PAGE_LINKS.add(link[\"href\"])\n","    save_data(save_file, list(PAGE_LINKS))                            # Saving the links.....\n","    return list(PAGE_LINKS)"]},{"cell_type":"markdown","metadata":{},"source":["> #### Process Main page"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["No Duplicate links found\n","Total 345 pages to be processed\n","Processing completed for 50 pages....\n","Processing completed for 100 pages....\n","Processing completed for 150 pages....\n","Processing completed for 200 pages....\n","Processing completed for 250 pages....\n","Processing completed for 300 pages....\n","Processing completed for 345 pages....\n"]}],"source":["main_page_url     = 'https://www.nvidia.com/en-us/'\n","link_file_name    = '1_NVIDIA_MAIN_PAGE_LINKS'\n","docs_file_name    = '1_NVIDIA_MAIN_PAGE_CONTENT'\n","\n","MAIN_PAGE_LINKS = get_links_main_page(main_page_url, link_file_name)   # get links\n","\n","working_page_links = MAIN_PAGE_LINKS.copy()                            # links to be processed\n","working_page_links = check_duplicate_links(working_page_links)\n","print(f\"Total {len(working_page_links)} pages to be processed\")\n","\n","MAIN_PAGE_DOCUMENT = read_content_from_pages(working_page_links, docs_file_name, 50)"]},{"cell_type":"markdown","metadata":{"id":"hU5EHX24HkrW"},"source":["> ### 2. NVIDIA Main document page        `https://docs.nvidia.com`"]},{"cell_type":"markdown","metadata":{},"source":["> #### Process Main Document page"]},{"cell_type":"code","execution_count":77,"metadata":{"id":"BOOyzTSYHj8G","outputId":"e58961d3-842d-43b9-994a-222a9e6b20ae"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing completed for 1 pages....\n"]}],"source":["working_page_links  = 'https://docs.nvidia.com'\n","docs_file_name      = '2_NVIDIA_MAIN_DOC_PAGE_CONTENT'\n","\n","MAIN_DOCUMENT_PAGE_DOCUMENT = read_content_from_pages([working_page_links], docs_file_name, 1)"]},{"cell_type":"markdown","metadata":{"id":"ybZu9saJZzKU"},"source":["> ### 3. NVIDIA All document pages        `https://docs.nvidia.com/#all-documents`"]},{"cell_type":"markdown","metadata":{},"source":["> #### Function: Get all href links from the page"]},{"cell_type":"code","execution_count":78,"metadata":{"id":"u8pJYZ8VC5xs"},"outputs":[],"source":["def get_links(url):\n","    page_links = set()\n","    html_text = requests.get(url).content\n","    soup = BeautifulSoup(html_text, 'lxml')\n","    links = soup.find_all(\"a\", class_=\"Link\", href=re.compile(\"https://\"))\n","    for link in links:\n","        page_links.add(link[\"href\"])\n","    return page_links"]},{"cell_type":"markdown","metadata":{},"source":["> #### Function: Get recursive links [upto 3rd level; Main URL -> link1s -> link2s -> link3s]"]},{"cell_type":"code","execution_count":79,"metadata":{"id":"xM0T0JEDC5u1"},"outputs":[],"source":["def get_recursive_links(url, save_file):\n","    PAGE_LINKS = set()\n","\n","    doc_pages = get_links(url)\n","    PAGE_LINKS = PAGE_LINKS.union(doc_pages)\n","\n","    for page in doc_pages:\n","        link_pages = get_links(page)\n","        PAGE_LINKS = PAGE_LINKS.union(link_pages)\n","\n","        for link_page in link_pages:\n","            next_link_pages = get_links(link_page)\n","            PAGE_LINKS = PAGE_LINKS.union(next_link_pages)\n","\n","    save_data(save_file, list(PAGE_LINKS))       # Saving the links.....\n","    return list(PAGE_LINKS)"]},{"cell_type":"markdown","metadata":{},"source":["> #### Process All documents page"]},{"cell_type":"code","execution_count":80,"metadata":{"id":"p77_kX4We57U","outputId":"94513d86-fc7d-41fd-c54a-b78ff3326d35"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of Duplicate links removed: 25\n","Total 32835 pages to be processed\n","Processing completed for 2000 pages....\n","Processing completed for 4000 pages....\n","Processing completed for 6000 pages....\n","Processing completed for 8000 pages....\n","Processing completed for 10000 pages....\n","Processing completed for 12000 pages....\n","Processing completed for 14000 pages....\n","Processing completed for 16000 pages....\n","Processing completed for 18000 pages....\n","Processing completed for 20000 pages....\n","Processing completed for 22000 pages....\n","Processing completed for 24000 pages....\n","Processing completed for 26000 pages....\n","Processing completed for 28000 pages....\n","Processing completed for 30000 pages....\n","Processing completed for 32000 pages....\n","Processing completed for 32835 pages....\n"]}],"source":["document_url   = 'https://docs.nvidia.com/#all-documents'\n","link_file_name = '3_NVIDIA_DOCUMENT_PAGE_LINKS'\n","docs_file_name = '3_NVIDIA_DOCUMENT_PAGE_CONTENT'\n","\n","DOC_PAGE_LINKS = get_recursive_links(document_url, link_file_name)    # get links\n","\n","working_page_links = []                                               # links to be processed\n","for link in DOC_PAGE_LINKS:\n","  if 'nvidia' in link.split(\".\"):\n","    working_page_links.append(link)\n","\n","non_working_links = [\n","'https://api.nvcf.nvidia.com',\n","'https://ngc.nvidia.com',\n","'https://developer.nvidia.com',\n","'https://catalog.ngc.nvidia.com',\n","'https://helm.ngc.nvidia.com/egxdefault',\n","'https://ets.egxdefault.egx.nvidia.com',\n","'https://fc.ngc.nvidia.com',\n","'https://nvid.nvidia.com',\n","'https://helm.ngc.nvidia.com/nvidia',\n","'https://eps.egx.nvidia.com',\n","'https://docs.nvidia.com/ai-enterprise/1.5/user-guide/index.html#configure-vmware-vsphere-vm-with-vgpuuser-guide/index.html',\n","'https://forums.developer.nvidia.com/c/healthcare/Parabricks/290',\n","'https://*.ngc.nvidia.com',\n","'https://egxdefault.egx.nvidia.com',\n","'https://docscontent.nvidia.com/sphinx/0000018c-87dc-d937-a99e-87deb3850000/0000018c-839d-de01-a38c-b3ff5b3d0000/cloud-functions/user-guide/latest/_downloads/2b19177fe9ae38d30a1c3da24edf09da/inference-test.zip',\n","'https://docscontent.nvidia.com/sphinx/0000018c-87dc-d937-a99e-87deb3850000/0000018c-839d-de01-a38c-b3ff5b3d0000/cloud-functions/user-guide/latest/_downloads/4f273de9a4ca3c4b8d1894c7b2f80907/postman.json',\n","'https://api.nvcf.nvidia.com/v3/openapi']\n","\n","for link in non_working_links:                                          # remove non working links\n","    if link in working_page_links:\n","        working_page_links.remove(link)\n","\n","working_page_links = check_duplicate_links(working_page_links)\n","print(f\"Total {len(working_page_links)} pages to be processed\")\n","DOCUMENT_PAGE_DOCUMENT = read_content_from_pages(working_page_links, docs_file_name, 2000)"]},{"cell_type":"markdown","metadata":{"id":"9jjAGSZ4Dq9D"},"source":["> ### 4. NVIDIA Forum pages               `https://forums.developer.nvidia.com/`"]},{"cell_type":"markdown","metadata":{},"source":["> #### Function: Get all href links from the page"]},{"cell_type":"code","execution_count":81,"metadata":{"id":"p7F5_eFTDtzj"},"outputs":[],"source":["def get_links_forum_pages(url, save_file):\n","    PAGE_LINKS = set()\n","\n","    html_text = requests.get(url).content\n","    soup = BeautifulSoup(html_text, 'lxml')\n","    divs = soup.find_all(\"div\", itemprop=\"itemListElement\")                     # Get all forums\n","    forums = set()\n","    for div in divs:\n","        path = div.find(\"meta\", itemprop=\"url\")\n","        forums.add('https://forums.developer.nvidia.com' + str(path.attrs['content']))\n","\n","    f_name_catagoies = set()\n","    for forum in forums:                                                        # Get the catagories from each forums\n","        html_text = requests.get(forum).content\n","        soup = BeautifulSoup(html_text, 'lxml')\n","        anchors = soup.find_all(\"a\", href=re.compile(\"/c/\"))\n","        for anchor in anchors:\n","            if forum[-5:] not in str(anchor):\n","               f_name_catagoies.add('https://forums.developer.nvidia.com' + str(anchor['href']))\n","\n","    for catagory in f_name_catagoies:                                           # Get the topics from each catagories\n","        html_text = requests.get(catagory).content\n","        soup = BeautifulSoup(html_text, 'lxml')\n","        sites = soup.find_all(\"a\", class_=\"title raw-link raw-topic-link\")\n","        for site in sites:\n","            PAGE_LINKS.add(str(site['href']))\n","\n","    save_data(save_file, list(PAGE_LINKS))                                      # Saving the links.....\n","    return list(PAGE_LINKS)"]},{"cell_type":"markdown","metadata":{},"source":["> #### Process Forum pages"]},{"cell_type":"code","execution_count":82,"metadata":{"id":"TCznNK9ADtsh","outputId":"9f3aede4-f524-4878-96c7-ae4fe3ee6c52"},"outputs":[{"name":"stdout","output_type":"stream","text":["No Duplicate links found\n","Total 3564 pages to be processed\n","Processing completed for 500 pages....\n","Processing completed for 1000 pages....\n","Processing completed for 1500 pages....\n","Processing completed for 2000 pages....\n","Processing completed for 2500 pages....\n","Processing completed for 3000 pages....\n","Processing completed for 3500 pages....\n","Processing completed for 3564 pages....\n"]}],"source":["forum_url      = 'https://forums.developer.nvidia.com/'\n","link_file_name = '4_NVIDIA_FORUM_PAGE_LINKS'\n","docs_file_name = '4_NVIDIA_FORUM_PAGE_CONTENT'\n","\n","FORUM_PAGE_LINKS = get_links_forum_pages(forum_url, link_file_name)     # get links\n","\n","working_page_links = FORUM_PAGE_LINKS.copy()                            # links to be processed\n","working_page_links = check_duplicate_links(working_page_links)\n","print(f\"Total {len(working_page_links)} pages to be processed\")\n","\n","FORUM_PAGE_DOCUMENT = read_content_from_pages(working_page_links, docs_file_name, 500)"]},{"cell_type":"markdown","metadata":{},"source":["> ### 5. NVIDIA CustomerHelp pages `https://nvidia.custhelp.com/app/answers/list/st/5/kw/grid`"]},{"cell_type":"markdown","metadata":{},"source":["> #### Function: Get all href links from the page"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[],"source":["def get_links_support_pages(url, save_file):\n","    PAGE_LINKS = set()\n","    for i in range (1, 6):\n","        URL = url + str(i)\n","        html_text = requests.get(URL).content\n","        soup = BeautifulSoup(html_text, 'lxml')\n","        links = soup.find_all(\"a\", href=re.compile(\"/app/answers/detail/a_id\"))\n","        for link in links:\n","            PAGE_LINKS.add(\"https://nvidia.custhelp.com\" + link[\"href\"])\n","    save_data(save_file, list(PAGE_LINKS))                            # Saving the links.....\n","    return list(PAGE_LINKS)"]},{"cell_type":"markdown","metadata":{},"source":["> #### Process CustomerHelp pages"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["No Duplicate links found\n","Total 46 pages to be processed\n","Processing completed for 10 pages....\n","Processing completed for 20 pages....\n","Processing completed for 30 pages....\n","Processing completed for 40 pages....\n","Processing completed for 46 pages....\n"]}],"source":["# Process NVIDIA Customer help pages:\n","custhelp_url   = 'https://nvidia.custhelp.com/app/answers/list/st/5/kw/grid/page/'\n","link_file_name = '5_NVIDIA_CUSTHELP_PAGE_LINKS'\n","docs_file_name = '5_NVIDIA_CUSTHELP_PAGE_CONTENT'\n","\n","CUSTHELP_PAGE_LINKS = get_links_support_pages(custhelp_url, link_file_name) # get links\n","\n","working_page_links = CUSTHELP_PAGE_LINKS.copy()                             # links to be processed\n","working_page_links = check_duplicate_links(working_page_links)\n","print(f\"Total {len(working_page_links)} pages to be processed\")\n","\n","CUSTHELP_PAGE_DOCUMENT = read_content_from_pages(working_page_links, docs_file_name, 10)"]},{"cell_type":"markdown","metadata":{},"source":["> ### 6. Wikipedia (NVIDIA) page          `https://en.wikipedia.org/wiki/Nvidia`"]},{"cell_type":"markdown","metadata":{},"source":["> #### Process Wikipedia page"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing completed for 1 pages....\n"]}],"source":["working_page_links  = 'https://en.wikipedia.org/wiki/Nvidia'\n","docs_file_name      = '6_NVIDIA_WIKIPEDIA_PAGE_CONTENT'\n","\n","WIKI_PAGE_DOCUMENT = read_content_from_pages([working_page_links], docs_file_name, 1)"]},{"cell_type":"markdown","metadata":{},"source":["> ### 7. Stack-Overflow pages for NVIDIA  `https://stackoverflow.com/ `"]},{"cell_type":"markdown","metadata":{},"source":["> #### Funtion: Get all href links from the page"]},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[],"source":["def stack_page_links(tag, page_end):\n","    page_links = set()\n","    for page in range (1, page_end + 1):\n","        url = \"https://stackoverflow.com/questions/tagged/\" + tag + \"?tab=votes&page=\" + str(page) +\"&pagesize=50\"\n","        html_text = requests.get(url).content\n","        soup = BeautifulSoup(html_text, 'lxml')\n","        links = soup.find_all(\"a\", class_=\"s-link\", href=re.compile(\"/questions/\"))\n","        for link in links:\n","            page_links.add(\"https://stackoverflow.com\" + link[\"href\"])\n","    print (f\"Links retrived for stckoverflow question tagged with: {tag}....\")\n","    return list(page_links)"]},{"cell_type":"markdown","metadata":{},"source":["> #### Function: Get content from each pages and create a text file for contents [use only top or 1st answer]"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[],"source":["def stack_get_qa(page_links):\n","    PAGE_QA_TEST = ''\n","    page_count = 0\n","    for page in tqdm(page_links):\n","        html_text = requests.get(page).content\n","        soup = BeautifulSoup(html_text, 'lxml')\n","        question_answer = soup.find_all(\"div\", class_=\"s-prose js-post-body\")\n","        if page_count % 250 == 0:\n","            time.sleep(5)\n","        if len(question_answer) < 2: # that means no answer to the question, remove that question/link\n","            pass\n","        else:\n","            PAGE_QA_TEST += question_answer[0].text + question_answer[1].text # 0 has question and 1 has answer\n","        page_count += 1\n","    print (f\"Processing completed for {page_count} pages....\")\n","    return PAGE_QA_TEST"]},{"cell_type":"markdown","metadata":{},"source":["> #### Function: Read text file and create Langchain Document"]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[],"source":["def read_content_from_stack_pages(text_file_name, save_file_name):\n","    page_loader = UnstructuredFileLoader(text_file_name,\n","                                         continue_on_failure = True, \n","                                         show_progress_bar=False,\n","                                         post_processors=[clean, clean_extra_whitespace, group_broken_paragraphs])\n","\n","    elements = page_loader.load()\n","    full_clean = elements[0].page_content\n","\n","    PAGE_CONTENT = [Document(page_content=full_clean, metadata={\"source\":'https://stackoverflow.com'})]\n","    print (f\"Processing completed for 1 file....\")\n","    save_data(save_file_name, PAGE_CONTENT)\n","    return PAGE_CONTENT"]},{"cell_type":"markdown","metadata":{},"source":["> #### Process Stackoverflow page"]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Links retrived for stckoverflow question tagged with: nvidia....\n","Links retrived for stckoverflow question tagged with: nvidia-jetson....\n","Links retrived for stckoverflow question tagged with: nvidia-jetson-nano....\n","Links retrived for stckoverflow question tagged with: nvidia-docker....\n","Links retrived for stckoverflow question tagged with: nvidia-digits....\n","Links retrived for stckoverflow question tagged with: nvidia-deepstream....\n","Links retrived for stckoverflow question tagged with: nvidia-smi....\n","Links retrived for stckoverflow question tagged with: nvidia-titan....\n","Links retrived for stckoverflow question tagged with: nvidia-flex....\n","Links retrived for stckoverflow question tagged with: nvidia-isaac....\n","Links retrived for stckoverflow question tagged with: nvidia-hpc-compilers....\n","Links retrived for stckoverflow question tagged with: nvidia-jetpack-sdk....\n","Links retrived for stckoverflow question tagged with: nvidia-sass....\n","Number of Duplicate links removed: 186\n","Total 4671 links to be processed\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4671/4671 [11:08<00:00,  6.99it/s]  \n"]},{"name":"stdout","output_type":"stream","text":["Processing completed for 4671 pages....\n","Processing completed for 1 file....\n"]}],"source":["link_file_name = '7_NVIDIA_STACK_OVERFLOW_QA_PAGE_LINKS'\n","docs_file_name = '7_NVIDIA_STACK_OVERFLOW_QA_PAGE_CONTENT'\n","stack_qa_text  = '7_NVIDIA_STACK_OVERFLOW_QA_TEXT.txt'\n","\n","tag_list = ['nvidia', 'nvidia-jetson', 'nvidia-jetson-nano', 'nvidia-docker', 'nvidia-digits', 'nvidia-deepstream', \n","            'nvidia-smi', 'nvidia-titan', 'nvidia-flex', 'nvidia-isaac', 'nvidia-hpc-compilers', 'nvidia-jetpack-sdk', 'nvidia-sass']\n","page_count = [74, 9, 6, 6, 2, 2, 1, 1, 1, 1, 1, 1, 1]\n","question_count = [3690, 401, 291, 268, 91, 56, 35, 12, 10, 10, 9, 5, 3]\n","\n","PAGE_LINKS = []\n","for tag, count in zip(tag_list, page_count):\n","    PAGE_LINKS.extend(stack_page_links(tag, count))\n","save_data(link_file_name, PAGE_LINKS)                            # Saving the links.....\n","\n","working_page_links = PAGE_LINKS.copy()                           # links to be processed\n","\n","working_page_links = check_duplicate_links(working_page_links)\n","print(f\"Total {len(working_page_links)} links to be processed\")\n","STACK_PAGE_QA_TEXT = stack_get_qa(working_page_links)\n","save_data(stack_qa_text, STACK_PAGE_QA_TEXT)\n","\n","STACK_PAGE_DOCUMENT = read_content_from_stack_pages(save_data_folder + stack_qa_text, docs_file_name)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
